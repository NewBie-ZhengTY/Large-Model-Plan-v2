{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "93f3e31c",
   "metadata": {},
   "source": [
    "激活函数：引入非线性+确保梯度传递(消失/爆炸)+能否收敛"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3756619",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 特征工程"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c20bf93",
   "metadata": {},
   "source": [
    "### VGG\n",
    "相比AlexNet框架更规则，更多的模块化\n",
    "得出更深更窄效果更好\n",
    "特别占内存\n",
    "VGG-N：N包括了全连接层\n",
    "### NiN\n",
    "网络中的网络：完全不要full\n",
    "NiN块：使用1*1的conv代替full\n",
    "架构：输入通道数是类别数，交替使用NiN块+Pool(stride=2),最后用全局avgPool代替full得到输出\n",
    "    不易overfit，更少参数个数\n",
    "### GoogLeNet\n",
    "9个inception块\n",
    "Inception块：四个路径从不同层面抽取信息，在输出通道维合并 \n",
    "更少参数个数 + 计算复杂度\n",
    "\n",
    "### 这些都是resnet的灵感？\n",
    "乘法变加法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "792f0d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#GoogLeNet 实现"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0832e28",
   "metadata": {},
   "source": [
    "批量归一化\n",
    "    全连接层作用在特征维，卷积层作用在通道维；\n",
    "    全&卷输出上，全&卷输入上，激活函数前\n",
    "    本质：向每个小批量内加入噪音，不用和dropout混用\n",
    "    会加速收敛(可用更大的lr)，但不改变精度\n",
    "\n",
    "如果一个数据集有 n 类，批量大小不要超过 10*n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17081a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#调包\n",
    "nn.BatchNormnd('输入维度')  # 选几d & 输入维度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af4ed011",
   "metadata": {},
   "outputs": [],
   "source": [
    "#判断张量是否在同设备上，设备对齐\n",
    "x = torch.randn(2, 2)\n",
    "X = torch.randn(2, 2).to(\"cuda\")\n",
    "\n",
    "print(x.device)  # cpu\n",
    "print(X.device)  # cuda:0\n",
    "\n",
    "print(x.device != X.device)  # True\n",
    "\n",
    "torch.backends.cudnn.benchmark = True   #（自动选择最快卷积算法）\n",
    "\n",
    "#小批量多GPU最后把梯度累加即可"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef15271b",
   "metadata": {},
   "source": [
    "AMP自动混合精度\n",
    "GradScaler 防止FP16时梯度过小导致下溢(变0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abeb2f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AMP + GradScaler\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "model = MyModel().cuda()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "scaler = GradScaler()\n",
    "\n",
    "for data, targets in dataloader:\n",
    "    data, targets = data.cuda(), targets.cuda()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    with autocast():  # 自动使用混合精度\n",
    "        outputs = model(data)\n",
    "        loss = loss_fn(outputs, targets)\n",
    "\n",
    "    scaler.scale(loss).backward()      # 放大 loss 并反向传播\n",
    "    scaler.step(optimizer)             # 等价于 optimizer.step()\n",
    "    scaler.update()                    # 动态调整缩放因子\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce7b4bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据增广（P36）"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
