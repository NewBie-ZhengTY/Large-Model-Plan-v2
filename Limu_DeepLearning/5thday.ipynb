{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa70d961",
   "metadata": {},
   "source": [
    "### 序列模型\n",
    "自回归：对(自身)见过的数据回归\n",
    "隐变量(马尔可夫模型)\n",
    "潜变量\n",
    "现在的很多大模型把 “先验” 加入到了模型 \"结构” 中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4a7dacb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 文本预处理\n",
    "#中文分词：结巴"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f0546dc",
   "metadata": {},
   "source": [
    "### 语言模型\n",
    "估计整个联合概率\n",
    "最常用是做预训练模型(BERT,GPT-3)，判断多个序列中哪个更常见(打字个性化，口语个性化)\n",
    "RNN的公式+模型要会画\n",
    "深度RNN\n",
    "双向RNN(这不就是bert么)：适合做序列特征提取 / 填空，不能用来推理\n",
    "    正反向隐藏成不同参数\n",
    "    两层是一组单元，也可以设计“深度双向”\n",
    "Transformer其实就是把传统语言模型的“分词后计算联合概率”集成在了架构上，不逐步显式估计条件概率\n",
    "\n",
    "### 梯度剪裁\n",
    "预防梯度爆炸\n",
    "\n",
    "中文文本语调算法\n",
    "\n",
    "追踪目标算法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31eecfaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 机器翻译数据集(数据预处理)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd82043f",
   "metadata": {},
   "source": [
    "编码器(特征提取)-解码器(回归)架构\n",
    "\n",
    "Seqtoseq\n",
    "    编码器是RNN，读取输入句子，可以是双向RNN\n",
    "    解码器是另一个RNN，训练时每个节点的输入用真正句子的词，推理时输入使用的是上一时态的输出\n",
    "    迁移学习，特征提取 中不对Encoder参数更新 ； 端到端的Trans(优化)需要更新梯度(可以更好适应任务)\n",
    "\n",
    "衡量序列生成好坏的BLEU\n",
    "\n",
    "生成图的软件 graffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b17d90e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Seqtoseq实现"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f398ea3c",
   "metadata": {},
   "source": [
    "束搜索(bin-search)\n",
    "    保存最好的k个序列，时间复杂度O(knT)\n",
    "    k=1是贪心；k=n是穷举\n",
    "\n",
    "注意力机制\n",
    "注意力分数：query和key的相似度（k是公司员工，v是员工薪水，v是自己）\n",
    "    1，可直接将query和key做内积\n",
    "    2，将query和key合起来进入一个单输出单隐藏层的MLP\n",
    "注意力权重：就是分数softmax的结果\n",
    "使用注意力机制的seqtoseq\n",
    "\n",
    "使用注意力机制的seqtoseq：更有效传递信息(根据De输出来匹配合适编码器的输出)\n",
    "\n",
    "自注意力(池化层)：O(n²d)\n",
    "位置编码矩阵(用三角函数定位):因为这样编码方便线性投影(通过一个固定的矩阵)，容易算相对位置信息\n",
    "    位置编码的p是自动生成的，不需要学习\n",
    "    可学习的相对位置编码就是bert\n",
    "    VIT位置编码怎么设计的\n",
    "\n",
    "如何通过nlp提取文本中问题对应的内容\n",
    "\n",
    "静态embedding：传统词向量模型每个词的embedding长度是固定的(不随上下文变动)\n",
    "动态embedding：LLM中的词向量模型embedding长度是变动的(随着上下文变动)\n",
    "\n",
    "transformer：\n",
    "    编码器纯基于self-attention没有RNN；\n",
    "    解码器中第一个att快通过掩码模拟RNN的时序性质，后边接正常att，key和value来自encoder\n",
    "    编码器 & 解码器结构一般是对称的\n",
    "    使用层归一化LN，针对样本元素(单一样本内部特征)进行归一化\n",
    "    预测第t+1个输出时：de中输入前t个预测值，在自注意力中，前t个预测值作为key和value，第t个预测值还作为query(要会画图)\n",
    "\n",
    "BERT\n",
    "    只用de，针对nlp的微调设计，模型搬过去(即便任务不同)几乎只改最后一个输出层，和超参\n",
    "    在微调时，不是底部的所有层都固定的，固定可 加速，不固定可以重新适配(效果更好)\n",
    "    任务\n",
    "        空缺词的预测\n",
    "        下一个句子的预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a7df2da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BERT 预训练\n",
    "import torch\n",
    "from torch inport nn\n",
    "\n",
    "# 预处理阶段\n",
    "def get_token_and_segments(tokens_a,tokens_b=None)\n",
    "    tokens = ['<cls>' + token_a + ['<sep>']\n",
    "    segments = [0] * (len(tokens_a)+2) #制造的seg列表用全是0填充的，长度是 token_a + 2\n",
    "    if tokens_b is not None:\n",
    "        tokens += tokens_b + ['<sep>']\n",
    "        segments +=[1]*(len(token_b)+1)\n",
    "    return tokens , segments\n",
    "\n",
    "# encoder\n",
    "class BERTEncoder(nn.Module)\n",
    "    def __init__(self,vocab_size,num_hiddens,norm_shape,fi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d3e802d",
   "metadata": {},
   "outputs": [],
   "source": [
    "self.P[:, :, 0::2] = torch.sin(x)    #将张量 x 的正弦值填入 self.P 张量的 偶数维度（0,2,4,…）通道 中。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a18db43",
   "metadata": {},
   "source": [
    "tableu数据在现实生活中更多\n",
    "tabula又是什么"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
