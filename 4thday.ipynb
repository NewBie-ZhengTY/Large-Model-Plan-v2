{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53a3ef04",
   "metadata": {},
   "outputs": [],
   "source": [
    "#目标检测\n",
    "#简单的边缘框可以用四个数字（左上+右下 / 右上+左下）定义，精细的怎么做？\n",
    "# COCO数据集（80个类别）\n",
    "#代码(属于画图范围)\n",
    "\n",
    "next(iter())    #next() 从迭代器中取下一个元素，常与iter()结合使用\n",
    "\n",
    "# 锚框\n",
    "# 先提出多个被称为锚框的区域(边缘框)，预测内部是否含有关注物体\n",
    "# 如果是，预测从这个锚框到真是边缘框的偏移\n",
    "#       用IoU计算相似度(J(A,B))\n",
    "#       训练时，每个锚框是一个训练样本(一般一张图有上万个)，要么标注成背景(所以会有大量负类样本)，要么关联上一个真实边缘框\n",
    "#       一张图片进来有9个锚框，就会有9个训练样本，训练时很可能是一张张图片读的\n",
    "#       被当作背景的锚框点对应的整个行 & 列会被设为背景，不再计算(控制丢弃率，提升泛化能力)\n",
    "#       去掉相似 / 重复的预测框 ， 使用非极大抑制(NMS)输出 \n",
    "\n",
    "#   (锚框)代码"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23adc7e1",
   "metadata": {},
   "source": [
    "### R(Region)-CNN（难实现）\n",
    "启发式搜索选锚框，SVM对类别分类，L-R模型来预测边缘框偏移  \n",
    "用最大池化控制形状\n",
    "### Fast R-CNN\n",
    "用CNN对整个(Faster的原因)图片抽特征  \n",
    "用RoI池化对每个锚框生成固定长度特征，直接送入全连接  \n",
    "精度损失：对小目标影响大\n",
    "### Faster R-CNN\n",
    "生成候选锚框步骤交给了NN\n",
    "Fast手动提框(有监督)，对每个框重复特征提取，开销大，使用RoIPooling  \n",
    "(快百倍)Faster自提框，共享整张图特征图，效率高；RoIPooling+Full\n",
    "### Mask RCNN\n",
    "对像素做标号，用FCN来利用这些信息\n",
    "### SSD(单发多框检测)\n",
    "每个像素生成多个以他为中心的多个锚框，通过多分辨率检测提升效果  \n",
    "底层检测小物体，上层检测大物体\n",
    "### YOLO\n",
    "SSD的基础上尽量不重叠锚框(加速)\n",
    "每个锚框预测B个边缘框"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "697e9e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SSD实现"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82f9f7a7",
   "metadata": {},
   "source": [
    "### 语义分割\n",
    "应用：背景虚化，路面分割，实例分割(目标检测进化版本)\n",
    "如何利用景深信息ladir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a80059f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 实例分割实现"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a48c052b",
   "metadata": {},
   "source": [
    "### 转置卷积\n",
    "可以增大输入的高宽，可以叫反卷积，De-conv / UpConv(相比反卷积，叫转置卷积更准确)\n",
    "常用于生成任务\n",
    "本质也是一种卷积(P47.2)\n",
    "\n",
    "### FCN\n",
    "用转置卷积层替换CNN最后的全连接层(参数变少降低计算量，适应各种尺寸，保留空间结构信息)，实现每个像素的预测\n",
    "\n",
    "### 样式迁移\n",
    "样式图片+内容图片"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bed5f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 转置卷积实现\n",
    "\n",
    "# 样式迁移实现\n",
    "import torchvision as tv \n",
    "import transforms as tf\n",
    "\n",
    "content_img = \n",
    "style_img = \n",
    "\n",
    "rgb_mean = torch.tensor([0.485,0.456,0.406])\n",
    "rgb_std = torch.tensor([0.229,0.224,0.225])\n",
    "\n",
    "def preprocess(img , image_shape):#图片变成tensor\n",
    "    transforms = tv.tf.Compose([\n",
    "        tv.tf.Resize(image_shape),\n",
    "        tv.tf.ToTensor()，\n",
    "        tv.tf.Normalize(mean=rgb_mean,std=rgb_std)\n",
    "    return tf(img).unsqueeze(0)  #在第0维度增加了1维，并且赋值为1；如果是unsqueeze(2)是在第2维维度值赋值为1(而不是数值)\n",
    "    #若原来张量为[1,2,3],在unsqueeze(0)后变成了[[1,2,3]] ; unsqueeze(1)后变成([[1],[2],[3]])\n",
    "    ])\n",
    "def postprocess(img):           #tensor变成图片\n",
    "    img = img[0].to(rgb_std.device)\n",
    "    img = torch.clamp(\n",
    "        img.permute(1,2,0)*rgb_std + rgb_mean,0,1) \n",
    "    return tv.tf.ToPILImage()(img) # 创建对象后调用函数\n",
    "    #将torch中标准化后的图像恢复为可显示形式(反标准化 + 裁剪像素)\n",
    "    #大或小于max / min的会被截断后变成max / min\n",
    "    # permute() 改变维度顺序\n",
    "    # view() 改变形状\n",
    "#抽取图像特征\n",
    "pretrained_net = tv.models.vgg19(pretrained = True)\n",
    "style_layers , content_layers = [0,5,10,19,28],[25]\n",
    "net = nn.Sequential(*[pretrained_net.features[i] for i in range(max(content_layers+style_layers)+1)])\n",
    "def extract_features(X,content_layers,style_layers):\n",
    "    contents = []\n",
    "    styles = []\n",
    "    for i in range(len(net)):\n",
    "        X = net[i](X)\n",
    "        if i in style_layers:\n",
    "            styles.append(X)\n",
    "        if i in content_layers:\n",
    "            contents.append(X)\n",
    "        return contents,styles\n",
    "def get_contents(image_shape,device):\n",
    "    content_X =\n",
    "    contents_Y,_ =          #空余“_”表示占位符，即不关心这个值\n",
    "def get_styles(image_shape,device):\n",
    "    style_X = \n",
    "    _,style_Y = \n",
    "    return style_X,style_Y\n",
    "\n",
    "# 定义损失(如何定义样式类似，匹配两个图片通道的统计信息,这里使用的gram矩阵)\n",
    "#风格转移的损失函数是内容损失，风格损失，总变化损失的加权和\n",
    "def content_loss(Y_hat,Y):\n",
    "    return torch.square(Y_hat - Y.detach()).mean()\n",
    "def gram(X):\n",
    "def style_loss(Y_hat,gram_Y):\n",
    "def tv_loss(Y_hat):             #tv就是图像平滑正则项(惩罚相邻像素的差异)，抑制图像噪声和不必要的高频细节\n",
    "def compute_loss():\n",
    "\n",
    "#初始化合成图像\n",
    "class SynthesizedImage(nn.Module):\n",
    "    def __init__(self.image_shape,**kwargs):\n",
    "        super(SynthesizedImage,self).__init__(**kwargs)\n",
    "        self.weight = nn.Parameter(torch.rand(*image_shape))\n",
    "    def forward(self):\n",
    "        return self.weight\n",
    "def get_inits(X,device,lr,styles_Y):\n",
    "\n",
    "# 训练\n",
    "def train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6227ec48",
   "metadata": {},
   "outputs": [],
   "source": [
    ".detach()     #返回一个不带梯度，不参与反向传播的新张量，它的数值和原张量相同\n",
    "x = torch.tensor([2.0], requires_grad=True)\n",
    "y = x * 3\n",
    "z = y.detach()\n",
    "print(z.requires_grad)   # False\n",
    "print(y.requires_grad)   # True\n",
    "\n",
    "tensor.pow(exponent)    #将 tensor 中的每个元素都 提升到 exponent 次幂。\n",
    "x = torch.tensor([2.0, 3.0, 4.0])\n",
    "y = x.pow(2)\n",
    "print(y)  # 输出: tensor([ 4.,  9., 16.])\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
